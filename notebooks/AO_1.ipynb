{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "AO_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3T28yooWxsJ"
      },
      "source": [
        "If your Dataset is stored on google drive, you have to connect the disk with notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDmRpfN-Yqf8"
      },
      "source": [
        "import matplotlib.image as img\r\n",
        "%matplotlib inline\r\n",
        "import numpy as np\r\n",
        "from collections import defaultdict\r\n",
        "from shutil import copy\r\n",
        "from shutil import copytree, rmtree\r\n",
        "import collections\r\n",
        "import os\r\n",
        "import random\r\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y-nS4PeWxsO",
        "outputId": "0c5b5337-1a3b-4ba6-8865-e34932cb435d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oZqRa7gWxsR"
      },
      "source": [
        "Create .txt files with image filenames. Separate file for original and fake images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erhoyEu4WxsV"
      },
      "source": [
        "!ls /content/drive/Shareddrives/datasets/animals/dataset/original/*.jpg > /content/drive/Shareddrives/datasets/animals/originals.txt\n",
        "!ls /content/drive/Shareddrives/datasets/animals/dataset/fake/fake*.jpg > /content/drive/Shareddrives/datasets/animals/fakes.txt\n",
        "\n",
        "original_filenames_path = \"/content/drive/Shareddrives/datasets/animals/originals.txt\"\n",
        "fake_filenames_path = \"/content/drive/Shareddrives/datasets/animals/fakes.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMz0vA6UWxsW"
      },
      "source": [
        "Paths to folders that include images. Current structure:\n",
        "* dataset/\n",
        " *        -original/\n",
        " *       -fake/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fL9mNUzWxsY"
      },
      "source": [
        "path = \"/content/drive/Shareddrives/datasets/animals/\"\n",
        "original_path = path + \"dataset/original\"\n",
        "fake_path = path + \"dataset/fake\"\n",
        "original_train_path = path + \"data/train/original\"\n",
        "fake_train_path = path + \"data/train/fake\"\n",
        "original_test_path = path + \"data/test/original\"\n",
        "fake_test_path = path + \"data/test/fake\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aI4-4kAWxsa"
      },
      "source": [
        "Function that creates array of filenames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWPT_k7aWxsc"
      },
      "source": [
        "def get_array_of_filenames(original_filenames_path, fake_filenames_path, main_path):\n",
        "    ori_paths = []\n",
        "    fake_paths = []\n",
        "  \n",
        "\n",
        "    with open(original_filenames_path, 'r') as file:\n",
        "        ori_paths = [main_path + read.strip() for read in file.readlines()]\n",
        "\n",
        "    with open(fake_filenames_path, 'r') as file:\n",
        "        fake_paths = [main_path + read.strip() for read in file.readlines()]\n",
        "\n",
        "\n",
        "    return ori_paths, fake_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhR9DujPWxsc"
      },
      "source": [
        "originals, fakes = get_array_of_filenames(original_filenames_path, fake_filenames_path, \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hh4b6SWWxsd"
      },
      "source": [
        "The dataset has 16000 images so we are using a part of it. This function takes random images from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGOjH799Wxse"
      },
      "source": [
        "def train_test_big_split(originals, fakes):\n",
        "    ori_len = len(originals)\n",
        "    fakes_len = len(fakes)\n",
        "\n",
        "    train_dataset_len = int(0.05 * ori_len)\n",
        "    test_dataset_len = int(0.1 * train_dataset_len)\n",
        " \n",
        "    ori_train_list = random.sample(range(0, ori_len), train_dataset_len)\n",
        "    fakes_train_list = random.sample(range(0, fakes_len), train_dataset_len)\n",
        "  \n",
        "    ori_train_files = [originals[i] for i in ori_train_list]\n",
        "\n",
        "    ori_test_files = []\n",
        "    ori_test_list = []\n",
        "    fakes_test_files = []\n",
        "    fakes_test_list = []\n",
        "\n",
        "    while len(ori_test_files) <= test_dataset_len :\n",
        "        rnd = random.randint(0, ori_len)\n",
        "        if (rnd not in ori_train_list and rnd not in ori_test_list):\n",
        "            ori_test_files.append(originals[rnd])\n",
        "        ori_test_list.append(rnd)\n",
        "  \n",
        "    fakes_train_files = [fakes[i] for i in fakes_train_list]\n",
        "  \n",
        "    while len(fakes_test_files) <= test_dataset_len :\n",
        "        rnd = random.randint(0, fakes_len)\n",
        "        if (rnd not in fakes_train_list and rnd not in fakes_test_list):\n",
        "            fakes_test_files.append(fakes[rnd])\n",
        "        fakes_test_list.append(rnd)\n",
        "  \n",
        "    return ori_train_files, ori_test_files, fakes_train_files, fakes_test_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruNf-mtnWxsh"
      },
      "source": [
        "ori_train, ori_test, fakes_train, fakes_test = train_test_big_split(originals, fakes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6-wac_eWxsh"
      },
      "source": [
        "This function creates file structure from our part of dataset that is neccessary for ImageDataGenerator.flow_from_directory() \n",
        "* data/\n",
        " *   -train/\n",
        "    *        -original/\n",
        "    *       -fake/\n",
        " *   -test/\n",
        "    *        -original/\n",
        "    *        -fake/\n",
        "    \n",
        "This may take a while because this method performs copying files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgqBUMlYWxsi"
      },
      "source": [
        "def prepare_files(ori_train, ori_test, fakes_train, fakes_test, src, ori_train_dest, fakes_train_dest, ori_test_dest, fakes_test_dest):\n",
        "    if not os.path.exists(os.path.join(ori_train_dest)):\n",
        "        os.makedirs(os.path.join(ori_train_dest))\n",
        "    if not os.path.exists(os.path.join(fakes_train_dest)):\n",
        "        os.makedirs(os.path.join(fakes_train_dest))\n",
        "    if not os.path.exists(os.path.join(ori_test_dest)):\n",
        "        os.makedirs(os.path.join(ori_test_dest))\n",
        "    if not os.path.exists(os.path.join(fakes_test_dest)):\n",
        "        os.makedirs(os.path.join(fakes_test_dest))\n",
        "\n",
        "  \n",
        "    for item in ori_train:\n",
        "        copy(os.path.join(item), os.path.join(ori_train_dest, item.split(\"/\")[-1]))\n",
        "    print(\"Copying original train data, done\")\n",
        "\n",
        "    for item in ori_test:\n",
        "        copy(os.path.join(item), os.path.join(ori_test_dest, item.split(\"/\")[-1]))\n",
        "    print(\"Copying original test data, done\")\n",
        "\n",
        "    for item in fakes_train:\n",
        "        copy(os.path.join(item), os.path.join(fakes_train_dest, item.split(\"/\")[-1]))\n",
        "    print(\"Copying fake train data, done\")\n",
        "\n",
        "    for item in fakes_test:\n",
        "        copy(os.path.join(item), os.path.join(fakes_test_dest, item.split(\"/\")[-1]))\n",
        "    print(\"Copying fake test data, done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbB9atMJWxsj",
        "outputId": "2114ef68-8ad9-4bc4-e012-3ff6af06ddfc"
      },
      "source": [
        "prepare_files(ori_train, ori_test, fakes_train, fakes_test, original_path, original_train_path, fake_train_path, original_test_path, fake_test_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying original train data, done\n",
            "Copying original test data, done\n",
            "Copying fake train data, done\n",
            "Copying fake test data, done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A4jcuMQWxsk"
      },
      "source": [
        "Now, we can move to the next script when proper analysis is performed."
      ]
    }
  ]
}